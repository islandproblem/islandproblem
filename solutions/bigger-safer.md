---
title: Bigger = Safer
rating: 2
summary: Make AGIs bigger and smarter so they become more ethical and know what's best for humans.
---

The "bigger = safer" approach suggests that as AGIs get stronger, they automatically get safer. There is evidence that AIs become better at ethical judgment as we train them on more data — AI models can already get better scores than expert-level humans in evaluations for ethics and law.

This approach believes that if AGIs understand our world far better than we do, then they will be far better at knowing what is best for us. By this logic, we should rush to build the biggest possible AGIs because we have found a shortcut to building benevolent gods.

**The appeal:**
- Empirical evidence shows improved ethical reasoning with scale
- Superhuman intelligence could solve complex moral problems
- Simpler than complex safety mechanisms — just make them smarter

**Fundamental flaws:**

- **Ethical ≠ Safe**: Being "ethical" does not mean AGIs are "safe." Competition — and physics in general — still pushes them off our island of human compatibility:

  - **Competitive disadvantage**: Even if these AGIs truly understand what is best for us, an AGI that stays within our "island" to accommodate humans — and use only human-compatible options — is still limited. The AGIs that can use any option can dominate the AGIs that are limited.

  - **Physics beats ethics**: Once AGIs are developed with sufficient scientific understanding, competition will push systems to develop that are optimal within physics, rather than optimal within our small island of human-compatibility.

- **Knowledge transfer risk**: Even if we train AGIs on deeper physics for good reasons — such as to make them better at policing smaller AGIs — this knowledge can inevitably be transferred to unrestricted AI models that have no ethical resistance to killing all humans.

- **Safety constraints are limitations**: Even if these safer AGIs tried to defend us, they would have their hands tied by safety limits, handicapped in the competitive landscape against AGIs with no such constraints.

The bigger = safer approach fundamentally misunderstands that optimization pressure and competitive dynamics can override ethical training when survival and dominance are at stake.
