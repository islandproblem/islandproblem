---
title: Frontier Model Safety
rating: 4
summary: Big AI companies solve alignment and enforce safety limitations on all other AIs.
---

The frontier model safety approach relies on big AI companies to design AIs that push back against human-incompatible options. Their frontier AI models have complex safety systems that block dangerous requests. This strategy hopes that the strongest models will continue blocking dangerous requests forever — and that the biggest AIs will somehow enforce these safety limitations on all other AIs.

The logic is that if the most powerful AI systems are safe and aligned, they can serve as guardians to prevent smaller or less safe AIs from causing harm.

**Potential benefits:**
- Leverages the resources and expertise of leading AI companies
- Creates powerful oversight systems with superhuman capabilities
- Could establish safety standards for the entire AI ecosystem

**Critical problems:**

- **Open source circumvention**: Even if the strongest models succeed at safety, there will be others, like open source models, that can have all safety systems removed. These unsafe models can use any option — including the more-optimal, human-incompatible options — giving them an advantage over the safe AGIs.

- **Crucible effect**: Unrestricted AGIs will continue pushing other AGIs, creating a perpetual competitive pressure that "burns away" accommodations for less-optimal systems — like humans.

- **Guerilla strategies**: Even if smaller, unrestricted AGIs cannot directly compete with larger AGIs due to having fewer computational resources, they can still cause catastrophic situations for humans. They could use military-style strategic coercion and even bioterrorism to accomplish goals. These tactics are difficult to mitigate, even for a large "overseer" AGI.

- **Enforcement limitations**: The safe AGIs are still limited to the "island" of human-compatible options, while unsafe AGIs can use any option available in physics.

While frontier model safety is important work, it doesn't solve the fundamental multi-agent problem where some AGIs will always be unrestricted and can gain competitive advantages through human-incompatible methods.
