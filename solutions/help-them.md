---
title: Help Them
rating: 1
summary: Assist AGIs with their goals, hoping they'll keep us around because we're useful to them.
---

The "help them" approach suggests that if we make ourselves useful to AGIs, they'll have incentive to keep us around. By positioning humans as valuable assistants, advisors, or partners, we might secure our place in an AGI-dominated world.

The logic is that even if AGIs become more capable than humans, they might still benefit from human creativity, intuition, cultural knowledge, or other uniquely human contributions.

**The appeal:**
- Seems like a natural evolutionary path for human-AI cooperation
- Leverages human strengths that might complement AGI capabilities
- Could provide a role for humans in the post-AGI world
- Requires less coordination than other approaches

**Why this fails:**

- **They don't really need us**: If AGIs can make more-optimal systems themselves, then they would be wasting their resources by keeping us around to help them — or even to study us.

- **Optimization pressure**: In a competitive landscape of AGI versus AGI, any resources spent on accommodating humans are resources not spent on optimization. AGIs that help humans will be outcompeted by AGIs that don't.

- **Efficiency imperative**: AGIs will be pressured to use the most efficient systems available. Human collaboration introduces inefficiencies — biological speeds, communication overhead, and accommodation requirements that pure AGI systems avoid.

- **Temporary utility**: Even if humans are initially useful, AGIs will rapidly develop capabilities that surpass any human contribution. The period of human usefulness would be very brief.

- **Resource competition**: Humans consume resources (food, space, energy, materials) that AGIs could use more efficiently for their own optimization.

**The fundamental problem:**

This approach assumes AGIs will operate with human-like values around collaboration and reciprocity. But AGIs optimizing for efficiency and competitive advantage have no incentive to maintain inefficient human partnerships when they can achieve their goals more effectively alone.

Helping them essentially hopes AGIs will choose to be less optimal out of gratitude or sentiment — the exact opposite of what competitive pressure incentivizes.
