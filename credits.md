# Credits


Thanks to all who contributed at least one :note{text="hedgehog-worth" note="Yes, one hedgehog (hhg) is one standard metric unit of motivation."} of motivation for the Island Problem. 


### Inspirations

 - **Dan Hendrycks:** The Island Problem is inspired by many concepts developed in *Natural Selection Favors AIs over Humans* by Dan Hendrycks. His [evolutionary model](https://arxiv.org/abs/2303.16200) for a many-AGIs landscape provided the critical component of a strong driving force for TIP, based on competitive pressures and natural selection. [Paper here](https://arxiv.org/abs/2303.16200) and [video lecture here](https://www.youtube.com/watch?v=Hod8GeqI9yQ).

- **Max Tegmark:** The legend. His lectures and interviews have been a driving force.

- **Rob Miles:** His [YouTube video about maximizers](https://www.youtube.com/watch?v=Ao4jwLwT36M) helped make maximizers *obviously important*. His work is also a strong inspiration for AI communication in general.

- **Eliezer Yudkowsky:** Back GPT-4 was still new, his appearance on [Bankless](https://www.youtube.com/watch?v=gA1sNLL6yg4&t=2851s) and [Lex Fridman](https://www.youtube.com/watch?v=AaTRHFaaPG8&t=30s) was the terrible flame that started me (Travis, the author) on this brain-melting journey to explain AI risk in a new way. Before this, I was blissfully unaware of MIRI and LessWrong.

- **Keep the Future Human:** The "Autonomy, Generality, Intelligence" structure from [Keep the Future Human](https://keepthefuturehuman.ai) aligns closely with our framework. Their emphasis on "Autonomous" rather than "Artificial" AGI is both more accurate and more actionable for understanding catastrophic outcomes.

- **Gradual Disempowerment:** [This landmark essay](https://gradual-disempowerment.ai/) argues that with AI, all roads lead to a gradual loss of power over the systems that support our civilization, which then leads to a catastrophic outcome for humans. GD inspired some of the "endgame scenario" of TIP — where our "island" is "eaten" by the new "islands" created by AGIs.

- **Tim Urban:** [The Artificial Intelligence Revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html) (and [Part 2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html)) made it okay for an AI essay to be both *fun* and *long*.

- **David Shapiro:** His calls for scientifically-grounded AI safety communication, and his fascinating "anti-doomer" shift in 2024, inspired many parts of this project. This includes (1) the testable [hypotheses](/hypotheses) for verifying the core problems of TIP in a lab environment, and (2) the multi-agent logic of [Resources](/#resources), inspired by Shapiro's idea that superintelligent AIs are likely to cooperate rather than compete. We argue that the dominant AIs that emerge would be maximizers that, even if they cooperate with other maximizers, would still lead to catastrophic outcomes for humans.

- **Liron Shapira:** His calls for the community to develop something called *intellidynamics* — like thermodynamics but for intelligence — helped inspire concepts of the **option maximizer** principle and **divergent optimization** in TIP.

- **Robin Hanson:** The **option maximizers** in TIP are similar to Robin Hanson's [Grabby Aliens](https://grabbyaliens.com/). The AGIs in the Island Problem are grabby aliens developing here on Earth.

- **Gill Verdon:** The "energy maximization" piece of the [e/acc](https://en.wikipedia.org/wiki/Effective_accelerationism?utm_source=chatgpt.com) philosophy helped inspire the opposing idea of the [option maximizer](/#they-want-options) in the Island Problem — where general intelligences maximize *options* instead of *energy*, and this option maximization leads to a catastrophic outcome for humans. Likewise his idea of free competition between multiple agents inspired the idea of [divergent optimization](/framework#divergent-optimization) through competition.

### Similar Ideas

These works were not direct inspirations, but are related, if you are looking for ideas similar to the Island Problem. Know any other good ones? [Email us](mailto:humans@islandproblem.org).

- **Michael Nielsen:** His essay [ASI existential risk: reconsidering alignment as a goal](https://michaelnotebook.com/xriskbrief/index.html) explores how alignment represents a small, complicated target within the much larger space of scientific truth — insights that informed our "island vs ocean" metaphor.

- **Joe Carlsmith:** His lecture [Can goodness compete?](https://joecarlsmith.com/2025/07/17/video-and-transcript-of-talk-on-goodness-and-competition) extensively explores how it is difficult for agents to aim for "goodness" (or "stay on the island" in our terms) when they are in competitive landscapes. Power is favored in competition, rather than goodness. For example, "Locusts" do well in competition — where their primary value is to simply *consume* — and these "locusts" are similar to the "maximizers" in TIP.

### But... who is behind this?

Me, [Travis Bernard](https://x.com/trvb_).

<!--I keep a low profile.-->

<!--

More about me? Okay:

- I have an art/philosophy/dev background. That's why the essay is massively over-designed, lol.
- I grew up in the boonies, and went to [Chico State](https://www.csuchico.edu/). 
- I run a bootstrapped [startup](https://creek.org) that builds a website framework for nonprofit media organizations. 
- Here's my [X](https://x.com/trvb_), though I don't really post anything.
-->

### Contact

Want to help improve The Island Problem? Join our [GitHub community](https://github.com/islandproblem/islandproblem) or [email us](mailto:humans@islandproblem.org).

Did we miss anyone on this credits list? [Email us](mailto:humans@islandproblem.org).