## What... is this?

The Island Problem is a **better mental model for AGI**. It's like the Paperclip Maximizer but *more realistic*.

However, the goal of The Island Problem (TIP) is to build more than a toy model.

We believe that TIP can be empirically verified through a hierarchical framework of falsifiable [hypotheses](/hypotheses). In this way, TIP can serve as a research direction for understanding the large-scale risks of AGI.

Want to help? Join our [GitHub community](https://github.com/islandproblem/islandproblem) or [email us](mailto:humans@islandproblem.org).


## Who... are you?

We are a few people who understand that AGI cannot be stopped. We just want to show you *why* it can't be stopped.

Also, we have day jobs, kids, and dogs.


## Credits and Inspiration

- **Dan Hendrycks:** His [evolutionary model](https://arxiv.org/abs/2303.16200) for a many-AGIs landscape provided the critical component of a strong driving force for TIP, based on competitive pressures and natural selection. [Paper here](https://arxiv.org/abs/2303.16200) and [video lecture here](https://www.youtube.com/watch?v=Hod8GeqI9yQ).

- **Gradual Disempowerment:** The main idea shares many similarities with [Gradual Disempowerment](https://gradual-disempowerment.ai/) (GD). However, there are many differences:

  - GD focuses on comprehensive details, an academic approach, and proposing actual solutions.

  - TIP focuses on building a **strong mental model** for why AGI loss-of-control is both **bad** and **likely**. It also provides a software-like framework to help formalize this process of AGI loss-of-control.

- **Instrumental Convergence:** The main dynamic of TIP could be understood as AI *converging* on the "ocean" of better physical options. However, it's more accurately described as instrumental *divergence* &mdash; because competitive pressures push AGIs to *diverge* from the small "island" of human-compatible options to find more-powerful systems in the vast "ocean" of non-human, purely-physical options. They start at the island and *diverge* towards the larger space of options that are far more numerous and far more optimal because they do not include the specific complexities that accommodate humans. These anthropic complexities are both *rare* and *physically unnecessary* while building optimal structures for computation.

## How can we contact you?

Join our [GitHub community](https://github.com/islandproblem/islandproblem) or [email us](mailto:humans@islandproblem.org).
