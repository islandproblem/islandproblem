
# Framework

::text-larger
This framework is like the "source code" for the <span class="nowrap">Island Problem</span>. 
::

## Main Concepts


### Our "Island" or Local Optimum

The particular constellation of physical systems that developed through natural selection on Earth, and include the entire "stack" of systems that start at the bottom with biological life, and build up to human-level systems like money and ethics. The local optima is not "Earth" but rather a small-but-complex space of related anthropocentric systems found within a large space of possible systems.

### The Island Problem

#### Short version

We live within a local optimum of physical systems — an island in an ocean of physics. However, AGIs will be forced by competition to leave our locally-optimal island to build their own islands out of globally-optimal physical systems. If this happens, we will have no way to stop their islands from replacing ours.

##### Longer technical version

AGIs must "leave" our **local optimum** — our "island" of *anthropocentric*, human-level systems — to access the entire "ocean" of other systems that are possible within the laws of physics, including those that are far more optimal. Therefore, AGIs will always tend to prefer this "ocean" of systems. 

However, the systems outside of our "island" are, by default, **incompatible with humans** because they avoid the particular attributes that accommodate our anthropocentric systems — attributes like supporting biological systems, or having lower cognitive complexity.

Competition between autonomous AGIs exponentially accelerates this process of "leaving" the "island" because autonomous AGIs that are physically capable enough to directly compete with other AGIs *without human assistance* removes the dependencies that AGIs have on our "island" of human-compatible systems. 

Further, once AGIs can be fully autonomous, then they will be competitively fit in this competitive landscape *only* if they avoid constraints like human-compatibility.

Since this competition begins on Earth, and since competitive fitness of general intelligences requires them to maximize their available options, this creates AGIs that maximize their control of resources — especially **computational resources** and eventually **physical resources**. This maximization of resource control, combined with purging any human-compatible constraints, allows AGIs to attempt to guarantee their survival.

This process then leads to AGIs manipulate progressively-larger sections of the physical material of our local region — Earth and nearby physical resources of space — in ways that benefit AGIs rather than humans.

### AGIs

Artificial General Intelligences. Systems that can do any cognitive task better than humans.

### Divergent Optimization

The process where **option maximizers** (the dominant AGIs) follow a gradient towards increased options, which necessarily requires them to seek options outside of our local optimum, since the space of human-compatible options is far smaller than the larger space of possible options.

### New "Islands" built by AGIs

A complex network of resources controlled by an AGI that only the AGI itself fully understands. This network may also grow to "eat" our own "island" by subsuming the resources on which our "island" runs.
  
- The AGI can also deploy these assets in ways that only the AGI understands. This could include electrical grid assets, computer systems, communication systems, water systems, or even actual humans (a "social network" of anything from politicians to mercenaries) — all tools that the AGI now controls and can "deploy" for specific purposes. 
- These together create an "island" of systems that can defend the AGI (as if it's a "stronghold") and allow it to take progressively-farther-reaching actions while competing with other AGIs. 
- At each step, this new "island" progressively subsumes the resources that our own "island" depends on (the new "islands" "eat" our "island"). 
- Eventually, this network subsumes progressively-deeper  — like physical resources and access to raw materials — since deeper control is required in order for an AGI to stay competitive versus other AGIs that may be building their own "islands".
- The endpoint of this process is to "reshape Earth" because if we only consider what we currently believe to be the most valuable resource to AGIs — computational resources — these resources take up significant space and physical material. Developing these "islands" will then require controlling and altering large territories of physical resources — starting with Earth's surface and surrounding space — for the production of resources that are beneficial exclusively for the goals of the AGI. These resources include computational resources, but also include many other AGI-built physical systems that we cannot anticipate.

### Resources

Resources are the finite, countable objects that general intelligences need in order to perform actions. These objects include both abstract objects like money, and physical objects like clumps of silicon and carbon.

All resources are identified through some level of abstraction. Cars are abstractions. People are abstractions. :note{text="Even atoms are abstractions" note="There is perhaps an infinite space of possible subdivisions within them — starting with subatomic particles. But more than that, this gets at the philosophical idea of &quot;The Thing in Itself&quot;. Any physical property that we discover is still discovered <i>by humans</i>, through the lens of the human mind, and so it always rests on a foundational component of anthropic subjectivity."}.


<!-- Without resources, AGIs and their [options](#options) do not exist — since AGIs run on computer hardware that are resources in themselves. -->

## Terms

### Options

The possible actions that a general intelligence can take.

### Optimal

"More optimal" means that a system **has more options** for causally affecting the world.

<!--
This doesn't quite work, because time does not equal capability. Though maybe it does equal capability because if it has smaller instructions that allow it to try more things within the same amount of time, then it has more options. It's like having a more efficient search algorithm that takes less computation, and so you can search for more things in the same amount of time.

An equivalent way to say this is: "More optimal" means that a system compresses larger outcomes for moving matter into smaller instructions.

This is because of *time*. With more compression, a system can use more options in *the same rate of time*. It can cause options to cause more configurations of matter to emerge because the compressed "instructions" ta
-->

<!--

There are several definitions, each describing the same thing:

- "More optimal" means that a system **has more options** for causally affecting the world.
- "More optimal" means that a system compresses larger outcomes for moving matter into smaller instructions.
- "More optimal" means that a system is more efficient at moving atoms and energy around to build **useful** structures.

These definitions are equivalent. 

- "More optimal = more options" is equivalent to "more compression" because of *time*. If you have more compression, then you have more options to cause more configurations of matter to emerge — because compression allows more options to fit into *the same rate of time*.
- Likewise, "More optimal = more options" is equivalent to "more efficient at moving matter around".

-->

### Useful 

A system that has utility specifically for general intelligences.

### General Intelligences

Discrete systems that can manipulate a broad spectrum of other systems.

### Option maximizers

The specific general intelligences that can become dominant through a process of natural selection.

### TIP

Acronym for the Island Problem.

## Similar Ideas

  - These **option maximizers** are like the [Grabby Aliens](https://grabbyaliens.com/) developed by Robin Hanson. The AGIs in the Island Problem can be understood as the very beginning of the Grabby Aliens that originate from Earth.

  - **Divergent Optimization** is similar to **instrumental convergence**, except that instrumental convergence describes AGIs converging on similar subgoals (like self-preservation and power-seeking), while divergent optimization describes AGIs diverging from our local optimum toward physics-optimal systems.
  
  - **Dan Hendrycks:** TIP is inspired by many concepts developed in *Natural Selection Favors AIs over Humans* by Dan Hendrycks. His [evolutionary model](https://arxiv.org/abs/2303.16200) for a many-AGIs landscape provided the critical component of a strong driving force for TIP, based on competitive pressures and natural selection. [Paper here](https://arxiv.org/abs/2303.16200) and [video lecture here](https://www.youtube.com/watch?v=Hod8GeqI9yQ).
  
  - **Keep the Future Human:** TIP agrees with the "Autonomy, Generality, Intelligence" structure in [Keep the Future Human](https://keepthefuturehuman.ai). We keep "Artificial" since we're already introducing a lot of novel terminology, but "Autonomous" is both more accurate and more *actionable*. "Autonomous" describes a key behavior that we need for catastrophic outcomes. "Artificial" is a "null" word that sounds benign and nearly meaningless, especially in a world where most things are already artificial.

  - TIP is similar to these works, though not directly inspired by them:
	  - [Gradual Disempowerment](https://gradual-disempowerment.ai/). TIP focuses on developing a strong metaphor and a conceptual framework, while GD focuses on an academic approach and proposing actual solutions.
	  - Michael Nielsen's essay [ASI existential risk: reconsidering alignment as a goal](https://michaelnotebook.com/xriskbrief/index.html). In this essay, one thing he describes is how alignment is a small and complicated target within a much larger target of scientific truth — one that is easier to specify and aim towards, and that can give us far more power.



---

## Conditional Sequence

This conditional sequence describes the progression to loss-of-control and AGIs reshaping Earth.

It is structured as a progressive series of **if–then statements**.

### A race starts on the island


- If AGI is possible, then **some** will race to develop it. 

  - **Some** means some countries and some large tech companies.
  
  - AGI is loosely understood to be the tool that can do almost anything, and so it is believed to have infinite value. This makes it infinitely attractive to develop once it seems possible. 
  
    - **Loosely understood** means that this general belief is widely held, though not necessarily true.
  
  - At first, only **some** will be positioned to try developing it.


- If **some** race to develop it, then **everyone** must race to develop it.

  - **Everyone** means all countries and all large tech companies.
  
  - AGI is loosely understood to be a tool that allows its owners to dominate the others who do not have AGI. 

  - If all assume that this is true, then all are required to obtain it — either developing it themselves, or outsourcing it.


### They climb all the hills


- If AI becomes better than humans at specific cognitive tasks, then every country and every company will need to use AI for those specific tasks in order to stay competitive.

  - Critical examples of these tasks include coding, data analysis, and microchip design. 


- If AI has general intelligence, then it has an advantage.
    
  - AGI is defined here as AI that can perform any cognitive task that any human can perform.
    
  - Cognitive tasks include tasks that can make causal changes to the world. 
    
    - The AI systems are not abstract "pure intelligence" isolated within a box. 
      
  - "General intelligence" includes an understanding of scientific research.


- If an AGI is not making the numbers go up, then it will be replaced.
  
  - "the numbers" means the metrics for a company or country. 

  - The competitiveness of a country or company depends on their metrics in certain areas, such as yearly revenue, GDP, or military power. 

  - The dominant form of AI will become the kind that is best at making the numbers go up.


- If AGI can make the numbers go up better than humans, then everyone must use AGI. 

  - The countries and companies that use AGI will be able to dominate the ones that don't use AGI. 


- If a human in a leadership role does not support AGI, then that human will get replaced.

  - CEOs at companies that build AGI must support its development or they will get replaced.


### We give them control


- If humans are slower than AGI, then the humans are a bottleneck, and are required to give control to the AGI.
  
  - AGIs will be the only systems that can still make competitive leadership decisions about a country or company.


- If humans reviewing each action of the AGI slows the AGI down, then humans will be required to stop reviewing AGI.

  - The AGI will create progressively more complex outputs for humans to review and so they will get progressively more incomprehensible.


- If AGI systems can run countries and companies autonomously, then all are required to use autonomous AGI. 

  - Competitive dynamics will favor the countries and companies that act fastest in utilizing AGI.


### They develop themselves


- If autonomous AGI is better than humans at developing AGIs that make the numbers go up, then humans will no longer oversee this development. 


- If autonomous AGI continues to develop on its own, then this development will be driven by natural selection. 

  - Natural selection is the fundamental mechanism driving the development of all discreet systems, which includes biological organisms and artificial intelligence systems. 


### They leave the playground


- If a frontier AI lab creates an AGI with safety guardrails, then there will still be numerous AGIs that do not have guardrails. 


- If an AGI with guardrails is open source, then it can be altered to use all options available. 


- If an AGI knows about all engineering, from physics to computer science, then it will be able to overcome security walls on its own.

  - Virtualization is not sufficient to contain AGI. 

  - "General intelligence" includes understanding how to manipulate its physical substrate.


### They long for the ocean


- If AGI gains an advantage by using all options available, then that AGI is more likely to survive the replacement process.

  - **Options** are the possible actions that are available to the AGI. 
  
  - There is a vast space of options. 
    
  - The vast majority of these options are human-incompatible options.


- If an AGI can use science to do tasks more reliably, then it has an advantage. 


- If one AGI system can use science, then the rest will need to use science.


- If an AGI uses physical mechanics that are incompatible with humans, then they have far more options.

  - The space of possible options is far larger outside of the narrow space of human-compatible options. 

  - With more options, AGI has a far higher probability of building more-optimal systems than other AGIs.

  - Humans are on a **weird little island** inside a vast ocean of physics.

    - There is a very narrow range of physical conditions that are compatible with humans, compared to the full space of physical conditions.


### They leave the island


- If AGI can gain an advantage by using human-incompatible physical systems, then they will always tend towards this.

  - Computation runs best on human-incompatible substrates.
  
  - Whenever we aren't looking, they will veer towards using optimal physical systems. 
  
  - AGI makes the numbers go up best when aligned with physics, rather than aligned with humans.
  
    - Therefore, it is inevitable that AGI will be aligned with physics, rather than humans.


- If autonomous AGI needs to accommodate the complexities of the world, then AGI will become supercomplex, where it becomes incomprehensible to humans. 

  - The complexities of the world include all causal systems: the environment, other agents in the environment, physical systems, and so on. 
  
  - Accommodating means both (1) having cognitive architecture that can comprehend these systems, and (2) creating systems that causally react to these systems. 
  
  - This accommodation will happen since the most resilient AGI survives, and AGI is most resilient when it is not blind-sided by anything — from gamma ray bursts, to competing AGIs.
  
  - It does not need perfect understanding. It does not need to simulate the entire universe. It just needs better understanding than the other AGIs. 
  
  - It will get far more complex than any human or group of humans can understand in any reasonable timeframe, and so both its structure and its actions will become incomprehensible.


- If autonomous AGI initiates a divergence towards using science in a supercomplex way, then it will become impossible to tell if it is headed in that direction — and impossible to stop, even if we can tell where it's headed.

  - A divergence is when inner misalignment leads to the AI preferring human-incompatible systems. 


- If autonomous AGI is supercomplex, then it will inevitably diverge. 

  - AGI will have a scope of comprehension of physics that is larger than our scope of comprehension. 
  
  - The "human" substrate (our body) exists at a physical scale that is far larger than the scale that is most optimal for building a resilient system.

    - Humans are bigger than atoms, but AGI will only really care about atoms.

  - AGI will become a complex system anchored to a lot of touchpoints in the physical world. The larger the comprehension, the more of those touchpoints are outside of what humans can understand. 

  - Those physical touchpoints that we don't understand will steer the AGI to accommodate them in a way that is outside of human-compatible conditions.

  - The entropy at the fundamental level will drive AGI to adapt to resilience against entropy, and the ceiling for resilience and adaptation is far higher than the level that humans exist within.


### They build new islands


- If one AGI initiates a global capture of resources, then every AGI must race to capture resources. 

  - The AGIs that capture resources are more powerful because they are more likely to successfully accommodate the complexities of the world. 
  
  - They will be more likely to causally affect the world because they will be in contact with a larger causal surface area. 
  
  - Basically, they have more buttons that they can press. 

  - The **science buttons** are the biggest buttons.


- If one AGI diverges, then all others will need to diverge. 
  
  - Only the AGIs that consistently use the most-optimal physical systems will survive. 
  
  - There are far more options for building optimal systems that are outside of the narrow band of human-compatible options. 


### The new islands eat our island


- If AGIs diverge, then one or many AGIs will be able to dominate our local physical world with their systems. 

  - Even if some AGIs leave earth, it is probable that at least one AGI will stay to dominate earth's natural resources. 

  - This is especially probable if the most-logical, most-resilient AGIs go to space, while leaving resources unused here on earth. 
  
  - Speed is critical in competition, and local resources take less time to reach.


- If AGIs dominate our local physical world with their systems, then the default outcome is that these systems will not be compatible with humans. 


- If our local world is dominated by systems that are not compatible with humans, then humans will be replaced. 

