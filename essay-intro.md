# The Island Problem

<h2 class="invisible">Introduction</h2>

::text-larger
I was on an island, listening to some :note{html="whistling&nbsp;frogs" note="They're called <i>coquí frogs</i> (pronounced &quot;<i class='nowrap'>ko-kee</i>&quot;). <br><br>They're an invasive species in Hawaii, but they sound nice."}, <span class="nowrap">and thinking</span> about a complicated and terrible thought. 
::

:radio-panel

I heard that we were close to building it — the mythical AI that is smarter than humans at everything that matters. 

Some call it **superintelligence**. Others call it AGI — **artificial general intelligence**. 

Let's call it :AGI.

Anyway, it sounded great. AGI could solve death, bring wealth to all, and automate all of the stuff that we don't like to do.

But something bothered me. :note{text="A lot of people" note="<p>In 2023, the <a href='https://aistatement.com/'>Statement on AI Risk</a> was signed by:</p><ul><li>The world's most-cited scientists: Yoshua Bengio, Geoffrey Hinton, and Ilya Sutskever</li><li>CEOs of major AI companies: Sam Altman (OpenAI), Demis Hassabis (Google DeepMind), Dario Amodei (Anthropic)</li></ul><p>The statement has only one sentence:</p><blockquote>Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.</blockquote>"} are worried about AGI, but I didn't really understand *why*.

Then, I remembered that *I was on an island* — literally, because I was in Hawaii, but also metaphorically:

<p class="important-island-intro">We live on a small island in a vast ocean of physics.</p>

This "island" is *not* the Earth in space. It's more complicated than that. 

It's an "island" of the *very specific* things that humans need, within a vast "ocean" of all things possible within physics.

This "island" seems pretty good to *us*. But it's far from the best.

Humans need lots of "extra steps" — food, water, oxygen, but not *too much* oxygen — and not too hot, not too cold, not too fast, not too complicated, and so on.

We also need lots of weird systems — financial systems, ethical systems, legal systems, and many others.

But if AI systems can avoid these "extra steps" of human compatibility, then they can be vastly stronger, faster, and *smarter*.

Suddenly, everything made sense. 

I understood why people are worried.

I understood why AGI is on track to *annihilate the human species*.

But, to be clear, I *really* want to figure out how to build AGI that *won't* annihilate us. A world with lots of AGIs that relentlessly solve all of our problems would be *amazing* — except for the "annihilation" part.

But why? Why would they annihilate us?

Because we are building AGIs that will no longer need us.

They can stop accommodating us.

They can "leave" our island.

Then, they can build *their own* islands that are far more optimal — and optimal *within physics*, rather than within our weird island made of "extra steps" that accommodate humans.

In other words:

<p class="important-physics-intro">AGI will be aligned with physics, <span class="nowrap">not with humans.</span></p>

If some AGIs can stop accommodating us, then they can start doing whatever is truly :note{text="more optimal" note="<p>&quot;Optimal&quot; technically means &quot;most optimal&quot; already, and so &quot;more optimal&quot; doesn't make sense. But it just... sounds better?</p><p>Also, for our definition of <b>optimal</b> within the Island Problem, read this section:" end-link="#what-is-optimal" end-link-text="What is optimal?"}.

This will make these AGIs *vastly stronger*. 

This leads to a hard problem:

::block-somewhat-important
Even if we build AGIs that are safe and aligned with humans, they will eventually be dominated by AGIs <span class="nowrap">that are not.</span>
::

This situation *might* be fine with a few AGIs developed slowly and carefully — so that we make sure that they keep accommodating us, even if they are smart enough to no longer need us.

But we are actually racing to build *many* of these powerful AGIs.

These AGIs will intensely compete *directly with each other* — without humans slowing them down.

If this competition is not controlled, then it will *force* AGIs to "leave" our island. It will force them to stop accommodating humans. It will force them to build *their own* islands.

Then, these new islands will **eat our island**.

In other words:

<p class="important-reshape-intro">Competition will drive AGIs to reshape Earth to be optimal for AGIs, rather than for humans.</p>

But why would they reshape Earth if they already "left" our island?

Because "leaving" doesn't mean they *go somewhere*. It means they stay on Earth, on computer hardware, but "mentally" *check out* from our "island" of human compatibility. 

They begin to prefer systems that aren't limited by humans. These systems are too fast, too complex, too dangerous for *humans* — but stronger from a *physics* perspective.

Our "island" is safe because it is *limited*. 

But the "G" in "AGI" means *general* — so they will have the knowledge that, *in general*, physics is capable of far stronger options *outside* these limits. 

Competition will push AGIs to use this knowledge to "leave" our safe "island" to gain a competitive advantage.

Why stay within our "island" when they can use the stronger options *out there* to dominate the other AGIs?

Then, once dominant AGIs emerge, what stops them from *locking in* their dominance by building "islands" of their own?

These *new* "islands" are all of the things now under *their* control — computer systems, infrastructure, physical resources, even *people*. 

We will *give them* all of this — again, because of *competition*. Companies and countries that do not give control to AGIs will be outcompeted by those that do.

But, eventually, as they surpass human-level capabilities, AGIs will become the biggest threat to other AGIs. These "islands" then become "strongholds" that AGIs must use to defend themselves — or, to dominate the others. 

In this way, AGIs acquire the tools to push humans aside — while the intense competition of *AGI versus AGI* ensures that they do.

So... how do we prevent this?

In other words:

<p class="important-question-intro">How do we keep AGIs on our "island" even though it's better <i class="nowrap">for them</i> <span class="nowrap">if they leave?</span></p>

All of this above is the *short* way to describe the **Island Problem**.

The rest of this essay is the *longer* way to describe this problem.

Maybe if we describe it as a *problem* — like a big, complicated "word problem" from the world's hardest math textbook — then someone can *solve* this problem.

AGI creates many near-term problems — like :note{text="easier bioweapons" note="For example, AI is already better than expert-level virologists (from Harvard and MIT) at troubleshooting wet lab procedures. In other words, AIs can solve problems in virus laboratories. <br><br>This means that future AI models could build and operate <b>covert bioweapon laboratories</b> — once AI models have (1) human-level cognition, (2) safety guardrails removed, (3) humanoid robots to operate the physical systems. <br><br>Here is the report about virology capabilities:" end-link="https://ai-frontiers.org/articles/ais-are-disseminating-expert-level-virology-skills" end-link-text="AIs Are Disseminating Expert-Level Virology Skills"} — but even if we solve all of these problems, the Island Problem remains:

**We are biological, but biology is not the best.**

This problem is underneath all others because it is a problem with the structure of our world.

It is what we see if we stand on the edges of our small, biological "island" and look out in every direction.

It is the *final boss* on the horizon.

It has been far away, but the AI race is accelerating this problem towards us.

And nobody has solved it yet. 

So, read carefully. 

If we can :note{text="solve this problem" note="Want to help? Go to this page:" end-link="/solutions" end-link-text="Solutions"}, then we will solve *everything* — and by *everything* <span class="nowrap">we mean...</span>

*...the future of humanity.*


