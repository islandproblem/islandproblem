# The Island Problem

<h2 class="invisible">Introduction</h2>

::text-larger
I was on an island, listening to some :note{html="whistling&nbsp;frogs" note="They're called <i>coquí frogs</i> (pronounced &quot;<i class='nowrap'>ko-kee</i>&quot;). <br><br>They're an invasive species in Hawaii, but they sound nice."}, <span class="nowrap">and thinking</span> about a complicated and terrible thought. 
::

:radio-panel

:AGI{html="<b>Artificial General Intelligence</b>"} seemed different from everything else. <span class="nowrap">I couldn't</span> quite describe *how* it was different, but I knew that it was. 

Then, I remembered that *I was on an island* — literally, because I was in Hawaii, but also metaphorically:

<p class="important-island-intro">We live on a small island in a vast ocean of physics.</p>

This "island" is *not* the Earth in space. It's more complicated than that. 

It's an "island" of everything that is compatible with humans — from our biological systems to our ethical systems.

However — to use a term from math and machine learning — our "island" is only a **local optimum** within a vast space of physics. This means that *our* optimum is not the *most* optimal. There are many other "islands" of physical systems that are :note{text="far more optimal" note="&quot;Optimal&quot; <i>technically</i> means &quot;most optimal&quot; already, and so &quot;more optimal&quot; doesn't make sense. But it just... sounds better? <br><br>It describes a realistic incremental process (&quot;continually more-optimal&quot;) rather than an unrealistic Platonic ideal (simply &quot;optimal&quot;).<br><br>Also, to define <b>optimal</b>: <br><br>The optimality of a general intelligence is based on how many <b>options</b> it has. This may seem a bit abstract, but we explain this throughout, and especially in the section titled &quot;<a href='#they-want-options'>They want options</a>&quot;."}.

<!-- Also, this "optimal" depends on how well its systems can move atoms around. It has nothing to do with humans.-->

Because of this, *I realized a problem*.

<!--
I realized that the people freaking out about AGI are right.

AGI will annihilate the human species if we stay on the current path.
-->
I realized that AGI is on track to be a *very bad thing*.

<!--But, to be clear, I *really* want to figure out how to build AGI that *won't* annihilate us. A world with lots of AGIs that relentlessly solve all of our problems would be *amazing*. It's just that they will create a bigger problem of their own.-->
But, to be clear, I *really* want AGI to be a good thing. A world with lots of AGIs that relentlessly solve all of our problems would be *amazing*. It's just that they will create a bigger problem of their own.

But, why?

Because, with general intelligence, these AGIs no longer need us. 

They can stop accommodating us.

They can *leave* our island. 

Instead, they can build *their own* islands that are far more optimal — and optimal *within physics*, rather than within our weird island of systems that have extra steps to accommodate us.

In other words:

<p class="important-physics-intro">AGI will be aligned with physics, <span class="nowrap">not with humans.</span></p>

<!--Physics allows for vastly more physical systems that are more powerful because they avoid the extra steps and limitations of humans needs. This means that even if AGIs are perfectly aligned with humans, they will eventually be outcompeted by AGIs that are not.-->

If AGIs can stop accommodating us, then they can start doing whatever is most optimal within physics.

This will make them *vastly stronger*. 

This means that even if we build AGIs that are safe and aligned with humans, they will eventually be dominated by AGIs that are not.

This situation *might* be fine with a few AGIs developed slowly and carefully — so that we make sure that they keep accommodating us, even if they are smart enough to no longer need us.

<!--  — so that we make sure that they keep accommodating humans, even if they are smart enough to no longer need us. -->

But we are actually racing to build *many* of these powerful AGIs that don't need our help anymore.

These AGIs will intensely compete *directly with each other* — without humans slowing them down.

<!--We will be *required* to develop these AGIs that *don't need us* because the countries and companies that have these AGIs can dominate the others.-->

<!--This *might* be fine with a few AGIs developed slowly and carefully. But there will actually be *many* AGIs, and they will intensely compete *directly with each other* — without humans slowing them down.-->

<!--This competition is also *almost* inevitable. We'll explain why inevitable, and why *almost*.-->

If this competition is not controlled, then it will *force* AGIs to leave our island. It will force them to stop accommodating humans. It will force them to build *their own* islands.

Then, these new islands will **eat our island**.

In other words:

<!--<p class="somewhat-important">Competition will drive AGIs to become optimal. This will lead AGIs to reshape Earth to be optimal for AGIs, rather than for humans. Then, AGIs will replace humans.</p>-->
<!--<p class="important-reshape-intro">Competition will force AGIs to race to be optimal. This will lead AGIs to reshape Earth to be optimal for AGIs, rather than for humans. Then, AGIs will replace humans.</p>-->
<p class="important-reshape-intro">Competition will drive AGIs to reshape Earth to be optimal for AGIs, rather than for humans.</p>
<!--<p class="important-reshape-intro">AGIs will reshape Earth for AGIs.</p>-->

<!--Human-compatible systems are a tiny fraction of all possible systems. The vast majority of ways to organize matter and energy completely ignore human needs — and they're more powerful because of it. This means that even perfectly aligned AGIs will eventually be outcompeted by AGIs that ignore human constraints.-->



<!--
How do we ensure that AGIs continue accommodating humans, despite intense pressure to use the most-optimal systems in a competitive landscape of AGI versus AGI?

-->

All of this is the *short* way to describe the **Island Problem**.

The rest of this essay is the *longer* way to describe this problem.

<!--It involves competition and countries, science and sandwiches.-->

Maybe if we describe it as a *problem* — like a big, complicated "word problem" from the world's hardest math textbook — then someone can *solve* this problem.

Nobody has solved it yet. So, read carefully. 

If you can :note{text="solve this problem" note="Want to help? Head over to the <a href='/solutions'>Solutions page</a>."}, then you will solve everything. And by *everything* <span class="nowrap">we mean...</span>

*...the future of humanity.*


