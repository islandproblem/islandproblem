# The Island Problem

Everything seems normal most of the time.

Something called the *anthropic principle* makes it seem this way.

But there is an idea hiding deep underneath what seems normal. This idea is like a small fish, deep in the ocean of physics, that you have to wait calmly and quietly to catch.

This is the idea:

**We live on a small island in an ocean of physics.**

This anthropic island has lots of special systems with extra steps that accommodate us.

Meanwhile, we melted some sand on our island and taught it to *think*. It went from **sand** to **silicon** to **microchips** to **neural networks**. 

Now, it can think *even better than us*. We're calling it AGI — artificial general intelligence.

AGIs that can solve all of our problems sounds *amazing*, but they create a bigger problem of their own.

With general intelligence, AGIs no longer need us. They can leave our island to create much better islands of their own.

These islands are better because they don't have the weird, extra steps that accommodate humans. They can be optimal within physics.

In other words:

<p class="important-physics">AGI will be aligned with physics, <span class="nowrap">not with humans.</span></p>

Ominous, right?

Here is the short way to describe the **Island Problem**:

<p class="somewhat-important">Competition will push AGIs to become optimal, which will force them to stop accommodating humans. Then, AGIs will reshape Earth to be optimal for AGIs, rather than humans — replacing humans in the process.</p>

The rest of this essay is the longer way of describing this problem.

It involves competition, countries, science, and sandwiches.

Maybe if we describe it as a problem — like a big, complicated "word problem" from the world's hardest math textbook — then someone can solve this problem.

Nobody has solved it yet.

So, read carefully. If you can solve this problem, then you will solve everything. And by *everything* we mean...

*...the future of humanity.*









<!--


Because of this, there are islands out there that can be far better than ours — islands that don't have these extra steps.


Everything seems normal most of the time.

But there is an idea hiding deep underneath what seems normal. This idea is like a small fish, deep in an ocean of physics, that you have to wait calmly and quietly to catch.



Physics is ancient. Humans are new.

When we arrived, physics had already built everything — stars, black holes, and oceans of things that we can't even imagine. 

But physics also left one tiny island where everything was just right for us. 

We've been playing in this sandbox ever since. 

However, it's very difficult to have an intuition about this. It's hard for us to understand that we live on a tiny Island and a vast ocean. The anthropic principle hides all of this from us.

We even taught the sand in our sandbox to *think*. We melted it into silicon and shaped it into computers. 

Now, it can think *even better than us*. We're calling it :AGI.

AGI that can solve all of our problems sounds *amazing*, but it creates a bigger problem of its own.

Here's the problem:

<p class="important-physics">AGI will be aligned with physics, <span class="nowrap">not with humans.</span></p>








Ominous, right? Yeah... well...

This is a special problem. This is the kind of problem that ends humanity. But it's also a problem that's almost as big as physics itself.

And we are building AGI right now, faster than any other project in human history, so we need to solve this problem *soon*.

But to understand why AGI won't stay in our sandbox — why it will leave our Island for the ocean, and why this is bad — we need to explain something about our world.

-->