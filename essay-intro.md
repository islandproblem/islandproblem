# The Island Problem

I was on an island, listening to some :note{text="whistling frogs" note="They're called <i>coquí frogs</i> (pronounced &quot;<i class='nowrap'>ko-kee</i>&quot;). <br><br>They're an invasive species in Hawaii, but they sound nice.<br><br>Like this:<br><br><audio controls loop><source src='https://cdn.freesound.org/previews/397/397998_7586039-lq.mp3'></audio>"}, and thinking about a complicated and terrible thought. 

:AGI{html="<b>Artificial General Intelligence</b>"} seems different from everything else. I couldn't quite describe *how* it was different, but I knew that it was. 

Then, I remembered that *I was on an island* — literally, because I was in Hawaii, but also metaphorically:

<p class="important-physics-intro">We live on a small island in a vast ocean of physics.</p>

This "island" is *not* the Earth in space. It's more complicated than that. 

It's an "island" of everything that is compatible with humans — from our biological systems to our ethical systems.

However — to use a term from math and machine learning — our "island" is only a **local optimum** within a vast space of physics. This means that *our* optimum is not the *most* optimal. There are many other "islands" of physical systems that are :note{text="far more optimal" note="&quot;Optimal&quot; <i>technically</i> means &quot;most optimal&quot; already, and so &quot;more optimal&quot; doesn't make sense. But it just... sounds better? It describes a realistic incremental process (&quot;continually more-optimal&quot;) rather than an unrealistic Platonic ideal (simply &quot;optimal&quot;).<br><br> Also, to define <b>optimal</b>: the optimality of an &quot;island&quot; is based on how well its physical systems can move atoms around to form specific useful structures — like computational substrate."}.

<!-- Also, this "optimal" depends on how well its systems can move atoms around. It has nothing to do with humans.-->

Because of this, *I realized a problem*.

I realized that AGI is on track to be a *very bad thing*.

To be clear, I *really* want AGI to be a good thing. A world with lots of AGIs that relentlessly solve all of our problems would be *amazing*, but they create a bigger problem of their own.

With general intelligence, these AGIs no longer need us. They can stop accommodating us.

They can *leave* our island. 

Instead, they can build *their own* islands that are far more optimal — and optimal *within physics*, rather than within our weird island of systems that have extra steps to accommodate us.

In other words:

<p class="important-physics-intro">AGI will be aligned with physics, <span class="nowrap">not with humans.</span></p>

This *might* be fine with a few AGIs developed slowly and carefully. But there will actually be *many* AGIs, and they will intensely compete *directly with each other* — without humans slowing them down. 

<!--This competition is also *almost* inevitable. We'll explain why inevitable, and why *almost*.-->

If this competition is not controlled, then it will *force* AGIs to leave our island. It will force them to stop accommodating humans. It will force them to build *their own* islands.

Then, these new islands will **eat our island**.

In other words:

<!--<p class="somewhat-important">Competition will drive AGIs to become optimal. This will lead AGIs to reshape Earth to be optimal for AGIs, rather than for humans. Then, AGIs will replace humans.</p>-->
<!--<p class="important-reshape-intro">Competition will force AGIs to race to be optimal. This will lead AGIs to reshape Earth to be optimal for AGIs, rather than for humans. Then, AGIs will replace humans.</p>-->
<p class="important-reshape-intro">Competition will drive AGIs to reshape Earth to be optimal for AGIs, rather than for humans.</p>

All of this is the *short* way to describe the **Island Problem**.

The rest of this essay is the *longer* way to describe this problem.

<!--It involves competition and countries, science and sandwiches.-->

Maybe if we describe it as a *problem* — like a big, complicated "word problem" from the world's hardest math textbook — then someone can *solve* this problem.

Nobody has solved it yet.

So, read carefully. If you can solve this problem, then you will solve everything. And by *everything* we mean...

*...the future of humanity.*


