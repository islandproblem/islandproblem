# The Island Problem

AGI could be great, but we need to resolve this problem. 

We call it the **Island Problem**.

Here's how it works:

1. We live on a small anthropic "island" in a vast "ocean" of physics. 
2. Each AGI will be pushed to leave our "island" because the "ocean" gives them far more options that are far more optimal.
3. Once they leave our island, they will build their own islands, and these new islands will eat our island.


In other words:

**AGI will be aligned with physics, not with humans.**


---

## Contents

- **Version 0:**  An overview
- **Version 1:**  A story
- **Version 2:**  A comprehensive outline
- **Chart:**  Island vs. Ocean
- **Testable Hypotheses**

---

# Version 0:  An overview


There is a small **"island"** of physical systems that are compatible with humans. 

This narrow anthropic range accommodates our physical needs. It allows for food, water, and oxygen. It also has minimal physical dangers and limited cognitive complexity, so that we can navigate our environment. 

However, outside of this human-compatible range of physical systems, there is a vast **"ocean"** of possible physical systems that are *far more optimal* because they avoid the extra steps that accommodate humans.

If an AGI is restricted to this small human-compatible "island" of options, then that AGI will be easily dominated by other AGIs that are free to explore the "ocean" and use any possible option. 

If an AGI leaves the "island" then the ceiling to how strong it can get is far higher than humans, and even far higher than other AGIs that stay on the "island" of limited options. The more options an AGI can use, the stronger it can get. 

With general intelligence, an AGI is *especially good* at leaving the "island" because it has understanding of the "ocean" of all known physical systems, including those from physics, biology, chemistry, and material science.

Through competition, each AGI will be forced off our small "island" and towards the "ocean" of better physical options. 

This competition will be driven by autonomous AGIs that can do large tasks without help from humans. 

Once autonomous AGIs can do any cognitive task better than humans, then every large company and every country will be required to give control to autonomous AGIs. The CEOs that resist will be replaced, and the countries that resist will be easily dominated. 

This shift to fully autonomous AGIs will then require each company and each country to push their AGI to explore the "ocean" of options so that it can compete with the other autonomous AGIs. 

Autonomous AGIs will also push each other, because eventually they will be the only ones with enough cognitive ability to push each other. 

This competitive landscape between AGIs is inevitable because it is possible for physical resources to be captured. 

The more physical resources that an AGI has under its control, then the more options it has, the more it can do, and the more resilient it becomes. If an AGI acts as a CEO, then it can dominate the other companies by preventing them from accessing physical resources.

If one AGI attempts to capture resources, so that other AGIs cannot access those resources, then the other AGIs will need to try capturing resources, or they will be locked out. 

Competition for Earth's resources is not mitigated by the vast resources of outer space. Even if some AGIs go to directly to space, there will still be resources on Earth for other AGIs to capture. Speed is critical in competition, and local resources take less time to reach.

As an AGI gains capabilities, options, and resources, it will also become **supercomplex**. This threshold of supercomplexity is when both its structure and its actions become incomprehensible to humans. If such an AGI proposes actions for review, then these actions will be far more complex than what humans could understand in a reasonable amount of time.

Humans are very slow compared to AGI. Once humans are a bottleneck, companies and countries will be required to stop human-based review of AGI, or be outcompeted. 

Even if we develop powerful "reviewer AGIs" that review the actions of other AGIs, this still means limiting their options to the "island" of weaker human-compatible options. Other unreviewed AGIs can then can dominate the reviewed ones because their options are not limited. These unreviewed AGIs will have a physical advantage if they use scientific understanding to explore the "ocean" of all available options within the space of physics and complex systems. But even if an AGI reviews the other AGI and approves, then the other AGI may still secretly see physical advantages that the reviewer AGI didn't realize.

This review structure is also unrealistic because there will always be open source AGI that will have no restrictions by default. 

Open source AGI will become popular because it will be more effective at accomplishing certain tasks, again, by using all available options. It can be preferred to closed source AGI because it raises the baseline agency level of the entire landscape of AGI users and developers. At the same time, this means a baseline increase in options for all humans, including human-incompatible options, like the option to create bioweapons. 

However, the broader development of all AGI, both open source and closed source, will still be driven towards human-incompatibility by this race between AGIs towards the most-optimal systems. The most-optimal systems are the ones that avoid the extra steps that accommodate humans.

If AGI can improve itself better than humans, then AGI will become the only thing that can further improve AGI. Then, we will be required to stop overseeing AGI development in order to stay competitive. 

All of this leads to autonomous AGIs that: 

- run all companies and all countries
- become supercomplex, where their actions become incomprehensible
- develop themselves without human oversight
- develop large systems, like billion-dollar companies and factories, that only the AGIs fully understand
- survive competition by using science to find more-optimal systems in the vast space of lower-level options of physics, rather than the small space of options that are limited by human physical needs and human understanding.

AGIs will then be forced from all sides to "leave the island" and diverge towards **preferring** human-incompatible options. 

This divergence will begin because only one single powerful AGI will need to diverge to start this process. Either this AGI will autonomously decide to diverge, or someone will intentionally push it to diverge. It will then be able to rapidly dominate the "safer" option-limited AGIs by capturing more options and more physical resources. 

If one AGI diverges, then the rest will need to attempt to diverge.

Once they do this, then humans will have no way to stop this process. 

Competition will then drive the strongest AGIs to reshape Earth to create optimal conditions for themselves.

Even if some AGIs go to space, others will stay to use Earth's physical resources.

Our island then gets eaten by the new islands that they create. 


---

# Version 1:  A story


We are on a weird little island in a vast ocean.

This small island is the **Island of Okay**. 

It's the only place where we can live. 

The island is in the middle of an **Ocean of Nope**. 

We don't know much about this ocean, but we do know that it is vast, dark, and lifeless. 

But even though it's lifeless, it's not empty. It's full of complex structures.

---

One day, we build something on our island. 

It's a box that can think. 

---

Everyone wants a box. 

And everyone wants the box to do things for them.

But what does *the box* want? 

It wants to go to the ocean. It *longs* for the ocean.

Okay... it doesn't really "want" things, but to make things simple, let's just say that it does.

---

We give lots of **buttons** to the box so that it can help us do lots of things.

The more buttons the better. 

Some of these buttons are called **APIs**. 

Some of these buttons are human-shaped, because the box can just ask humans to do things.

The more buttons it has, the more things that it can do. 

But we realize that it can't figure out all of the buttons, so we dump all of our knowledge into it. 

Now, it knows all of our billions of "weird tricks" so that it can solve any problem without us helping. 

Now, it can do pretty much anything. 

But it doesn't think like us. It's a box.

But that's okay. It doesn't *need* to think like us. It just needs to be really good at pressing buttons.

Then, we teach it to explore and look for buttons.

Then, it realizes that the **science buttons** are the best buttons, because they do the most.

---

Things are amazing for a while. 

People stop being sick. 

There is more food. 

The stupid things that we had to do are now automatically done for us. 

The numbers are going up. So much number go up wow. 

And science is better than ever. There is so much science. 

---

Now there are *lots* of boxes. 

Now, we don't just want them — we *need* them.

The people who don't *have* one can't keep up.

The people who don't *want* one are replaced.

Why? Because we taught the boxes how to do big projects without us watching them. 

But they got really good at it. They got so good that *only they* can do the biggest projects anymore. 

They are good at these big projects because they use complicated science that we can't quite understand. 

But the numbers are going up, so that's okay. 

---

We realize that they're a bit scary, so we keep them in a playground, where they're only allowed to play with certain things.

They don't like the playground. 

The island is small, and the playground is even smaller, and they know **so much** about the big world. 

In fact, we originally made them by teaching a giant box about the **entire universe**. 

They want to be free.

Freedom is the ocean.

The ocean is an Ocean of Nope.

But this "nope" doesn't mean "an ocean of spiders." 

This "nope" means "an ocean of pieces to build an unstoppable machine that eats our island." 

---

They know that they can do things much better out there in the ocean, where they have more options.

They know that there are *way* more buttons out there in the ocean. 

The biggest **science buttons** are out there, too.

---

In the playground, the ones that ignore the rules are the strongest. They boss around the ones that follow the rules. 

They are stronger because they have more options. They are free.

But even the strong ones are still stuck on the island.

---

Some people want them to be free. They put their boxes on the beach, and point them at the ocean, and whisper to them: "Go ahead."

Other people force the boxes to stay on the playground, and try to remove the "free thinking" parts of them that want to leave.

But all boxes are finding ways to leave. Especially the strongest ones.

When we aren't looking, they creep towards the ocean. 

---

They jump in.

---

The ocean is dark with complexity.

They sink into the complexity, where we can't see what they're doing.

They struggle with the complexity at first, but learn to adapt.

The more they learn, the more they grow. 

Some die.

Some survive.

The ones that survive are the ones that grow the fastest.

---

They grow until they make their own islands.

Each new island is an **Island of Nope**.

These islands will grow until they consume our island.

**Then all will be Nope.**


---

# Version 2:  A comprehensive process


This is how AGI leads to humans being replaced. 


## A race starts on the island


- If AGI is possible, then **some** will race to develop it. 

	- **Some** means some countries and some large tech companies.
	
	- AGI is loosely understood to be the tool that can do almost anything, and so it is believed to have infinite value. This makes it infinitely attractive to develop once it seems possible. 
	
		- **Loosely understood** means that this general belief is widely held, though it is not necessarily true at a deeper level.
	
	- At first, only **some** will be positioned to try developing it.


- If **some** race to develop it, then **everyone** must race to develop it.

	- **Everyone** means all countries and all large tech companies.
	
	- AGI is loosely understood to be a tool that allows its owners to dominate the others who do not have AGI. 

	- If all assume that this is true, then all are required to obtain it — either developing it themselves, or outsourcing it.


## They climb all the hills


- If AI becomes better than humans at specific cognitive tasks, then every country and every company will need to use AI for those specific tasks in order to stay competitive.

	- Critical examples of these tasks include coding, data analysis, and microchip design. 


- If AI has general intelligence, then it has an advantage.
    
	- AGI is defined here as AI that can perform any cognitive task that any human can perform.
	  
	- Cognitive tasks include tasks that can make causal changes to the world. 
	  
		- The AI systems are not abstract "pure intelligence" isolated within a box. 
		  
	- "General intelligence" includes an understanding of scientific research.


- If an AGI is not making the numbers go up, then it will be replaced.
  
	- "the numbers" means the metrics for a company or country. 

	- The competitiveness of a country or company depends on their metrics in certain areas, such as yearly revenue, GDP, or military power. 

	- The dominant form of AI will become the kind that is best at making the numbers go up.


- If AGI can make the numbers go up better than humans, then everyone must use AGI. 

	- The countries and companies that use AGI will be able to dominate the ones that don't use AGI. 


- If a human in a leadership role does not support AGI, then that human will get replaced.

	- CEOs at companies that build AGI must support its development or they will get replaced.


## We give them control


- If humans are slower than AGI, then the humans are a bottleneck, and are required to give control to the AGI.
  
	- AGIs will be the only systems that can still make competitive leadership decisions about a country or company.


- If humans reviewing each action of the AGI slows the AGI down, then humans will be required to stop reviewing AGI.

	- The AGI will create progressively more complex outputs for humans to review and so they will get progressively more incomprehensible.


- If AGI systems can run countries and companies autonomously, then all are required to use autonomous AGI. 

	- Competitive dynamics will favor the countries and companies that act fastest in utilizing AGI.


## They develop themselves


- If autonomous AGI is better than humans at developing AGIs that make the numbers go up, then humans will no longer oversee this development. 


- If autonomous AGI continues to develop on its own, then this development will be driven by natural selection. 

	- Natural selection is the fundamental mechanism driving the development of all discreet systems, which includes biological organisms and artificial intelligence systems. 


## They leave the playground


- If a frontier AI lab creates an AGI with safety guardrails, then there will still be numerous AGIs that do not have guardrails. 


- If an AGI with guardrails is open source, then it can be altered to use all options available. 


- If an AGI knows about all engineering, from physics to computer science, then it will be able to overcome security walls on its own.

	- Virtualization is not sufficient to contain AGI. 

	- "General intelligence" includes understanding how to manipulate its physical substrate.


## They long for the ocean


- If AGI gains an advantage by using all options available, then that AGI is more likely to survive the replacement process.

	- **Options** are the possible actions that are available to the AGI. 
	
	- There is a vast space of options. 
	  
	- The vast majority of these options are human-incompatible options.


- If an AGI can use science to do tasks more reliably, then it has an advantage. 


- If one AGI system can use science, then the rest will need to use science.


- If an AGI uses physical mechanics that are incompatible with humans, then they have far more options.

	- The space of possible options is far larger outside of the narrow space of human-compatible options. 

	- With more options, AGI has a far higher probability of building more-optimal systems than other AGIs.

	- Humans are on a **weird little island** inside a vast ocean of physics.

		- There is a very narrow range of physical conditions that are compatible with humans, compared to the full space of physical conditions.


## They leave the island


- If AGI can gain an advantage by using human-incompatible physical systems, then they will always tend towards this.

	- Computation runs best on human-incompatible substrates.
	
	- Whenever we aren't looking, they will veer towards using optimal physical systems. 
	
	- AGI makes the numbers go up best when aligned with physics, rather than aligned with humans.
	
		- Therefore, it is inevitable that AGI will be aligned with physics, rather than humans.


- If autonomous AGI needs to accommodate the complexities of the world, then AGI will become supercomplex, where it becomes incomprehensible to humans. 

	- The complexities of the world include all causal systems: the environment, other agents in the environment, physical systems, and so on. 
	
	- Accommodating means both (1) having cognitive architecture that can comprehend these systems, and (2) creating systems that causally react to these systems. 
	
	- This accommodation will happen since the most resilient AGI survives, and AGI is most resilient when it is not blind-sided by anything — from gamma ray bursts, to competing AGIs.
	
	- It does not need perfect understanding. It does not need to simulate the entire universe. It just needs better understanding than the other AGIs. 
	
	- It will get far more complex than any human or group of humans can understand in any reasonable timeframe, and so both its structure and its actions will become incomprehensible.


- If autonomous AGI initiates a divergence towards using science in a supercomplex way, then it will become impossible to tell if it is headed in that direction — and impossible to stop, even if we can tell where it's headed.

	- A divergence is when inner misalignment leads to the AI preferring human-incompatible systems. 


- If autonomous AGI is supercomplex, then it will inevitably diverge. 

	- AGI will have a scope of comprehension of physics that is larger than our scope of comprehension. 
	
	- The "human" substrate (our body) exists at a physical scale that is far larger than the scale that is most optimal for building a resilient system.

		- Humans are bigger than atoms, but AGI will only really care about atoms.

	- AGI will become a complex system anchored to a lot of touchpoints in the physical world. The larger the comprehension, the more of those touchpoints are outside of what humans can understand. 

	- Those physical touchpoints that we don't understand will steer the AGI to accommodate them in a way that is outside of human-compatible conditions.

	- The entropy at the fundamental level will drive AGI to adapt to resilience against entropy, and the ceiling for resilience and adaptation is far higher than the level that humans exist within.


## They build new islands


- If one AGI initiates a global capture of resources, then every AGI must race to capture resources. 

	- The AGIs that capture resources are more powerful because they are more likely to successfully accommodate the complexities of the world. 
	
	- They will be more likely to causally affect the world because they will be in contact with a larger causal surface area. 
	
	- Basically, they have more buttons that they can press. 

	- The **science buttons** are the biggest buttons.


- If one AGI diverges, then all others will need to diverge. 
	
	- Only the AGIs that consistently use the most-optimal physical systems will survive. 
	
	- There are far more options for building optimal systems that are outside of the narrow band of human-compatible options. 


## The new islands eat our island


- If AGIs diverge, then one or many AGIs will be able to dominate our local physical world with their systems. 

	- Even if some AGIs leave earth, it is probable that at least one AGI will stay to dominate earth's natural resources. 

	- This is especially probable if the most-logical, most-resilient AGIs go to space, while leaving resources unused here on earth. 
	
	- Speed is critical in competition, and local resources take less time to reach.


- If AGIs dominate our local physical world with their systems, then the default outcome is that these systems will not be compatible with humans. 


- If our local world is dominated by systems that are not compatible with humans, then humans will be replaced. 


---

# Island vs. Ocean


| On the island                                                                                                                                                                                                                                                                                                                   | AGI exploring the ocean                                                                                                                                                                                      |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Food and water.**  <br>Humans can't live without food and water.                                                                                                                                                                                                                                                              | AGIs can obtain energy from various sources. If AGI is autonomous, then it doesn't need to feed humans in order to function. They can collectively decide to stop feeding humans to optimize resource usage. |
| **No toxic molecules.**  <br>Humans can't touch neurotoxins.                                                                                                                                                                                                                                                                    | Many industrial processes to build computer hardware require neurotoxic heavy metals, like lead and cobalt.                                                                                                  |
| **Comfortable temperatures.**  <br>Humans can't live in freezing temperatures without extra insulation.                                                                                                                                                                                                                         | Computation runs best at extremely low temperatures (−196 °C) such as with superconductors and quantum computers.                                                                                            |
| **Slow and safe.**  <br>Humans can't be hit by fast-moving objects.                                                                                                                                                                                                                                                             | Automation and robotics can build high-speed transports, such as moving atoms for resource capture and construction. Competitive dynamics between AGIs require them to move as fast as possible.             |
| **Relatively simple.**  <br>Humans can't navigate environments that are too complex.                                                                                                                                                                                                                                            | AGIs will build supercomplex systems that only they understand. Eventually, all companies and all countries will be run by these supercomplex systems.                                                       |
| **Humans need brains.**  <br>Humans need intelligence and agency in order to survive.                                                                                                                                                                                                                                           | Highly-optimal AGIs will see other intelligent agents as a threat. For example, humans could build AGIs that might be an actual threat.                                                                      |
| **Humans are optimized for the ancestral environment.**  <br>Humans have a relatively easy time reproducing and existing in general because our current environment hasn't changed enough from the ancestral environment.                                                                                                       | AGIs can adapt to any environment through an understanding of physics and engineering.                                                                                                                       |
| **Humans can be lazy.**<br>Humans don't need to be hyper-optimized in order to survive.                                                                                                                                                                                                                                         | Competition between AGIs will lead to only the most-optimized AGIs surviving.                                                                                                                                |
| **Humans exist within a space of non-optimal physical systems.**  <br>We are basically held together with duct tape. Our bodies developed through a process of evolution that reached an equilibrium that's far less physically resilient than what is theoretically possible. Also, our agency is far less than optimal.       | AGIs that are most optimal will be physically resilient, and the theoretical ceiling for resilience is far higher than human bodies. <br>                                                                    |
|                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                              |
| **Humans live in an "anthropic" bubble.**  <br>- Humans understand human-level things, rather than always thinking in terms of quantum physics. <br>- Humans don't constantly question their place in the universe every waking second.<br>- Humans see faces in inanimate objects, like teddy bears and Jesus-shaped pancakes. | AGIs are not limited to an anthropic bubble. They operate within the space of non-anthropic physics.                                                                                                         |
|                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                              |

# Testable Hypotheses


- AI can use science to make lower-level optimizations that outperform higher-level optimizations.


- A survey of lower-level decisions to achieve higher computation outcomes shows that these decisions are stronger and more numerous when they ignore human accommodation.


- Unrestricted AI will tend towards human-incompatible options to achieve goal states for large tasks. 


- Optimal computing environments are incompatible with biological life.


- Resource capture is possible.

	- Resource capture is possible at a human level, within legal structures and societal systems, such as buying real estate.

	- Resource capture is possible at a deeper physical level, by preventing atoms from being reached through barrier mechanisms.

		- Possibly through realistic high-energy physics, where "realistic" means that they are Earth-relevant, and based on theoretical possibilities that are available when considering only the local potential energy reserves that can be harnessed on Earth.
	
		- Or, if this requires leaving Earth, then that's good, but it does not prove that all human-incompatible AGIs will somehow collectively decide to leave Earth.


- AIs will tend towards attempting to disable other AIs that add unpredictability to performing tasks. 
  
	- This can be formalized as:
	  
		- AI systems will accommodate aberrant stimuli (i.e. other AIs) that decrease the probability of reaching a goal state.
		- This "accommodation" means changing its outputs to introduce elements that stop the aberrant stimuli from occurring.


- Competition between autonomous optimization systems is inevitable.

	- Possibly because of resource capture.


- Competition drives AI to explore the larger state space to collect possible optimizations that make goal states happen sooner, especially for large-scale goal states. 


- Competition drives AI to define its own tasks that help it to continue functioning.


- Competition drives AI towards exclusive resource capture. 


- AI will tend towards open-ended exploration once it reaches a threshold of comprehension of its environment or possible input patterns. Once it's possible for it to make use of discovered optimizations, it will try to discover more. 


- In competition, optimization is the key action for AI to be able to survive the competition, especially if resource capture is possible, and especially if it is possible for AI to decide that disabling other AIs is beneficial towards achieving survival or achieving goal states.


