# The Island Problem

> AGI will be aligned with physics, not with humans.

There is a small **"island"** of physical systems that are compatible with humans. 

This narrow anthropic range accommodates our physical needs. It allows for food, water, and oxygen. It also has minimal physical dangers and limited cognitive complexity, so that we can navigate our environment. 

<!-- <img class="image-header" alt="Illustration of the Island Problem." src="/images/island-and-new-island.svg" /> -->

However, outside of this human-compatible range of physical systems, there is a vast **"ocean"** of possible physical systems that are *far more optimal* because they avoid the extra steps that accommodate humans.

If an AGI is restricted to this small human-compatible "island" of options, then that AGI will be *easily dominated* by other AGIs that are free to explore the "ocean" and use any possible option. 

If an AGI *leaves* the "island" of limited options, then it can become stronger than other AGIs that stay on the "island" of limited options. The more options an AGI can use, the stronger it can get. 

With **general intelligence**, an AGI is *especially good* at leaving the "island" because it has understanding of the "ocean" of all known physical systems, including those from physics, biology, chemistry, and material science.

Through **competition**, each AGI will be forced off our small "island" and towards the "ocean" of better physical options. 

This competition will be driven by **autonomous AGIs** that can do large tasks without help from humans. 

Once autonomous AGIs can do any cognitive task better than humans, then every large company and every country will be required to give control to autonomous AGIs. The CEOs that resist will be replaced, and the countries that resist will be easily dominated. 

This shift to fully-autonomous AGIs will then require each company and each country to push their AGI to explore the "ocean" of options so that it can compete with the other autonomous AGIs. 

Autonomous AGIs will also push each other, because eventually they will be the only ones with enough cognitive ability to push each other. 

This competitive landscape between AGIs is inevitable because they can **capture resources**. 

If one AGI attempts to capture resources, then the other AGIs will need to try capturing resources, or they will be locked out. 

The more resources that an AGI has under its control, then the more options it has, the more it can do, and the more resilient it becomes. If an AGI acts as a CEO, then it can dominate the other companies by preventing them from accessing resources.

These resources include **human-level resources**, like computer systems, companies, legal structures, and people.

But this process inevitably leads to capturing **physical resources**, like atoms and energy, because they allow for a *theoretical maximum* level of optimization. Systems built from purely-physical resources can dominate systems built from human-level resources because they are not weighed down by the extra steps to accommodate humans.

AGI that has general intelligence about all scientific research will be especially good at capturing these physical resources. 

If one AGI uses its general intelligence of science to capture as many resources as possible, then the other AGIs will need to follow. Otherwise, both the other AGIs *and* their companies or countries will be locked out of resources, and permanently dominated by those with the most physical resources.

Competition for Earth's resources is not mitigated by the vast resources of outer space. Even if some AGIs go directly to space, there will still be resources on Earth for other AGIs to capture. Speed is critical in competition, and local resources take less time to reach.

As an AGI gains capabilities, options, and resources, it will also become **supercomplex**. This threshold of supercomplexity is when both its structure and its actions become incomprehensible to humans. If such an AGI proposes actions for review, then these actions will be far more complex than what humans could understand in a reasonable amount of time.

Humans are very slow compared to AGI. Once humans are a bottleneck, companies and countries will be *required* to **stop human-based review** of AGI, or be outcompeted. 

Even if we develop powerful "reviewer AGIs" that review the actions of other AGIs, this still means limiting their options to the "island" of weaker human-compatible options. Other *unreviewed* AGIs can then dominate the reviewed ones because their options are not limited. These unreviewed AGIs will have a physical advantage if they use scientific understanding to explore the "ocean" of all available options within the space of physics and complex systems. But even if an AGI reviews the other AGI and approves, then the other AGI may still secretly see physical advantages that the reviewer AGI didn't realize.

This review structure is also unrealistic because there will always be **open source AGI** that will have no restrictions by default. 

Open source AGI will become popular because it will be more effective at accomplishing certain tasks, again, by using all available options. At a societal level, it can be preferred to closed source AGI because it raises the baseline agency level of the entire landscape of AGI users and developers. At the same time, this means a baseline increase in *options* for all humans, including human-incompatible options, like the option to create bioweapons. 

However, the broader development of all AGI, both open source and closed source, will still be driven towards human-incompatibility by this race between AGIs towards the most-optimal systems. The most-optimal systems are the ones that avoid the extra steps that accommodate humans.

If AGI can improve itself better than humans, then AGI will become the only thing that can further improve AGI. Then, we will be required to stop overseeing AGI development in order to stay competitive. 

All of this leads to autonomous AGIs that: 

- run every large company and every country
- become supercomplex, where their actions become incomprehensible
- develop themselves without human oversight
- develop large systems, like billion-dollar companies and militaries, that only the AGIs fully understand
- survive competition by using the *far more optimal* systems found in the vast space of physics, rather than only using the small space of weaker systems that accommodate humans

AGIs will then be forced from all sides to "leave the island" and diverge towards **preferring** human-incompatible options. 

This **divergence** away from our "island" will begin because only one single powerful AGI will need to diverge to start this process. Either this AGI will autonomously decide to diverge, or someone will intentionally push it to diverge. It will then be able to rapidly dominate the "safer" option-limited AGIs by capturing more options and more physical resources. 

If one AGI diverges, then the rest will need to attempt to diverge, or be locked out of resources.

Once they do this, then humans will have no way to stop this process. 

Competition will then drive the strongest AGIs to reshape Earth to create optimal conditions for themselves.

Even if some AGIs go to space, others will stay to use Earth's physical resources.

Our island then gets eaten by the new islands that they create. 
