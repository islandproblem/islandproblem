# The Island Problem

> AGI will be aligned with physics, not with humans.

![Illustration of the Island Problem](/images/we-live-here.svg)

We live on a small **"island"** of physical systems that are compatible with humans. 

This narrow **anthropic** range accommodates our human needs. 

![Our Island](/images/human-island.svg)

It supports biological life, and allows for food, water, and oxygen. 

It contains all of our social systems, computer systems, companies, countries, governments, laws, football, and sandwiches.

It's a bit simpler. It has limited cognitive complexity, so that we can navigate our environment without getting stuck.

It's a bit safer. It has minimal physical dangers, so that we are not killed by toxic molecules, radiation, extreme temperatures, or fast-moving pieces of metal. All of the rectangles have rounded corners.

![Ocean of Physics](/images/ocean-of-physics.svg)

However, outside of this human-compatible range of physical systems, there is a vast **"ocean"** of possible physical systems that are *far more optimal* because they avoid the extra steps that accommodate humans.

Think of these physical systems as options.

The more options an AGI can use, the stronger it can get.

For an AGI, these options are like **buttons** that it can press. It doesn't need consciousness. It just needs to be really good at pressing buttons. "Really good" means that when it presses these buttons, they can actually changes things in the real world.

![Money is easy now!](/images/money-is-easy-now.svg)

If an AGI is restricted to this small human-compatible "island" of options, then that AGI will be *easily dominated* by other AGIs that are free to explore the "ocean" and use any possible option. 

If an AGI *leaves* the "island" of limited options, then it can become stronger than other AGIs that stay on the "island" of limited options.

With **general intelligence**, an AGI is *especially good* at leaving the "island" because it has understanding of the "ocean" of all known physical systems, including those from physics, biology, chemistry, and material science.

**Competition** will force AGIs to leave our small "island" and start exploring the "ocean" of better physical options. 

**Autonomous AGIs** will drive this competition because these AGIs can do large tasks without help from humans.

Once autonomous AGIs can do any cognitive task better than humans, then every large company and every country will be *required* to give control to autonomous AGIs. The CEOs that resist will be replaced, and the countries that resist will be easily dominated. 

This shift to fully-autonomous AGIs will then require each company and each country to push their AGI to explore the "ocean" of options so that it can compete with the other autonomous AGIs.

But more importantly, autonomous AGIs will **push each other** because eventually AGIs will be *the only ones* with enough cognitive ability to push other AGIs. 

This competitive landscape between AGIs is inevitable because they can **capture resources**. 

If one AGI attempts to capture resources, then the other AGIs will need to try capturing resources, or they will be locked out. 

The more resources that an AGI has under its control, then the more options it has, the more it can do, and the more resilient it becomes. If an AGI acts as a CEO, then it can dominate the other companies by preventing them from accessing resources.

These resources include **human-level resources**, like computer systems, companies, legal structures, and people.

But this process inevitably leads to capturing **physical resources**, like atoms and energy, because they allow for a *theoretical maximum* of optimization. Systems built from purely-physical resources can dominate systems built from human-level resources because they are not weighed down by the extra steps to accommodate humans.

AGI that has general intelligence about all scientific research will be especially good at capturing these physical resources.

If one AGI uses its general intelligence of science to capture as many resources as possible, then the other AGIs will need to follow. Otherwise, both the other AGIs *and* their companies or countries will be locked out of resources, and permanently dominated by those with the most physical resources.

Competition for Earth's resources is not mitigated by the vast resources of outer space. Even if some AGIs go directly to space, there will still be resources on Earth for other AGIs to capture. Speed is critical in competition, and local resources take less time to reach.

As an AGI gains capabilities, options, and resources, it will also become **supercomplex**. This threshold of supercomplexity is when both its structure and its actions become incomprehensible to humans. If such an AGI proposes actions for review, then these actions will be far more complex than what humans could understand in a reasonable amount of time.

Humans are very slow compared to AGI. Once humans are a bottleneck, companies and countries will be *required* to **stop human-based review** of AGI, or be outcompeted. 

Even if we develop powerful "reviewer AGIs" that review the actions of other AGIs, this still means limiting their options to the "island" of weaker human-compatible options. Other *unreviewed* AGIs can then dominate the reviewed ones because their options are not limited. These unreviewed AGIs will have a physical advantage if they use scientific understanding to explore the "ocean" of all available options within the space of physics and complex systems. But even if an AGI reviews the other AGI and approves, then the other AGI may still secretly see physical advantages that the reviewer AGI didn't realize.

This review structure is also unrealistic because there will always be **open source AGI** that will have no restrictions by default. 

Open source AGI will become popular because it will be more effective at accomplishing certain tasks, again, by using all available options. At a societal level, it can be preferred to closed source AGI because it raises the baseline agency level of the entire landscape of AGI users and developers. At the same time, this means a baseline increase in *options* for all humans, including human-incompatible options, like the option to create bioweapons. 

However, the broader development of all AGI, both open source and closed source, will still be driven towards human-incompatibility by this race between AGIs towards the most-optimal systems. The most-optimal systems are the ones that avoid the extra steps that accommodate humans.

If AGIs can **improve themselves** better than humans, then AGIs will become the only thing that can further improve AGIs. Then, we will be required to *stop overseeing AGI development* in order to stay competitive. 

All of this leads to autonomous AGIs that: 

- run every large company and every country
- become supercomplex, where their actions become incomprehensible to humans
- develop themselves without human oversight
- develop large systems, like billion-dollar companies and militaries, that only the AGIs fully understand
- develop superhuman understandings of physical systems by training on simulations and scientific data
- develop a competitive landscape of AGI versus AGI, where humans no longer participate
- survive competition by using the *far more optimal* systems found in the vast space of physics, rather than only using the small space of weaker systems that accommodate humans

With these conditions in place, AGIs will then be forced from all sides to "leave our island" and diverge towards *preferring* human-incompatible options.

If some AGIs diverge, then the *entire competitive landscape of AGIs* will diverge.

This **divergence** will be possible if one AGI gains enough *agency* and enough *scientific understanding* to shift its primary choice of resources away from the weaker space of *human-level* resources and towards the more-optimal space of *purely-physical* resources. This AGI may still use some human-level resources, but they will no longer be its primary choice.

Either this AGI will diverge on its own, or someone will intentionally push it to diverge, with the hope that it will help their company or country dominate the others.

This AGI will **self-reinforce** this divergence because it will find itself far more successful in the competitive landscape of AGIs once it can *write its own rules* within the larger space of physics.

It will then be able to rapidly dominate the safer, option-limited AGIs by using *any* option, including human-incompatible options, to capture the most physical resources.

If one AGI diverges, then the rest will need to attempt to diverge, or be locked out of resources.

Once this divergence begins, humans will have no way to stop this process.

**AGI will be aligned with physics, not with humans.**

After this, even if AGIs choose *cooperation* over competition, it will be AGIs cooperating with other AGIs, and not with humans. Those AGIs that cooperate with humans would be limited by human systems, and dominated by AGIs that use physical systems that are more optimal.

Competition for physical resources will then drive the dominant AGIs to continue maximizing their dominance by **reshaping Earth** to create "islands" of optimal conditions for themselves. They will build strongholds to defend their dominance.

Even if some AGIs go to space, others will stay to build their islands from Earth's physical resources.

Our island then gets eaten by the new islands that they create.